{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the year for which you need to fetch the log files: 2008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1% 2695168 / 2691676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for  log20080101.zip not present in cache. Downloading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0% 9568256 / 9568033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for  log20080201.zip not present in cache. Downloading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0% 5062656 / 5061203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for  log20080301.zip not present in cache. Downloading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0% 13705216 / 13698946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for  log20080401.zip not present in cache. Downloading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0% 16818176 / 16815379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for  log20080501.zip not present in cache. Downloading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1% 5013504 / 5009430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for  log20080601.zip not present in cache. Downloading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1% 10240000 / 10232407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for  log20080701.zip not present in cache. Downloading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0% 9068544 / 9067321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for  log20080801.zip not present in cache. Downloading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1% 6250496 / 6244934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for  log20080901.zip not present in cache. Downloading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0% 14270464 / 14264659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for  log20081001.zip not present in cache. Downloading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1% 6840320 / 6834973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for  log20081101.zip not present in cache. Downloading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0% 14229504 / 14227372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for  log20081201.zip not present in cache. Downloading data\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import requests\n",
    "import os.path\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "def maybe_download(url_list, year):\n",
    "   \n",
    "    list=['df1','df2','df3', 'df4','df5','df6','df7','df8','df9','df10','df11','df12']\n",
    "    year=str(year)\n",
    "    count=0\n",
    "    for i in url_list:\n",
    "        \n",
    "        #fetching the zip file name from the URL\n",
    "        file_name=i.split(\"/\")\n",
    "        create_directory(\"Part_2_log_datasets_trial/\"+year+\"/\")\n",
    "        \n",
    "        #Downloading data if not already present in the cache\n",
    "        if(os.path.exists(\"Part_2_log_datasets_trial/\"+year+\"/\"+file_name[8])):\n",
    "            print(\"Data for \",file_name[8],\" is already present, pulling it from cache\")\n",
    "\n",
    "        else:\n",
    "            #pbar = ProgressBar(widgets=[Percentage(), Bar()])\n",
    "  \n",
    "\n",
    "            urllib.request.urlretrieve(i, \"Part_2_log_datasets_trial/\"+year+\"/\"+file_name[8], reporthook)\n",
    "            print(\"Data for \",file_name[8],\"not present in cache. Downloading data\")\n",
    "\n",
    "        #unzip the file and fetch the csv file\n",
    "        zf = zipfile.ZipFile(\"Part_2_log_datasets_trial/\"+year+\"/\"+file_name[8]) \n",
    "        csv_file_name=file_name[8].replace(\"zip\", \"csv\")\n",
    "        zf_file=zf.open(csv_file_name)\n",
    "        \n",
    "        #create a dataframe from the csv and append it to the list of dataframe\n",
    "        list[count]=pd.read_csv(zf_file)\n",
    "        count=count+1  \n",
    "    \n",
    "def create_directory(path):\n",
    "        try:\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "        except OSError as exception:\n",
    "            if exception.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "def generate_url(year):\n",
    "    url_list=list()\n",
    "    #generate the url for fetching the log files for every month's first day\n",
    "    number_of_months=1\n",
    "    \n",
    "    while number_of_months < 13:\n",
    "        #find the quarter for the month\n",
    "        if number_of_months >= 1 and number_of_months < 4:\n",
    "            quarter=\"Qtr1\"\n",
    "        elif(number_of_months >= 4 and number_of_months < 7):\n",
    "            quarter=\"Qtr2\"\n",
    "        elif(number_of_months >= 7 and number_of_months < 10):\n",
    "            quarter=\"Qtr3\"\n",
    "        elif(number_of_months >= 10 and number_of_months < 13):\n",
    "            quarter=\"Qtr4\"\n",
    "\n",
    "        if(number_of_months <10):\n",
    "            url=\"http://www.sec.gov/dera/data/Public-EDGAR-log-file-data/\"+str(year)+\"/\"+quarter+\"/log\"+str(year)+'%02d' % number_of_months+\"01.zip\"\n",
    "            \n",
    "        else:\n",
    "            url=\"http://www.sec.gov/dera/data/Public-EDGAR-log-file-data/\"+str(year)+\"/\"+quarter+\"/log\"+str(year)+str(number_of_months)+\"01.zip\"\n",
    "\n",
    "        url_list.append(url)\n",
    "        number_of_months=number_of_months+1\n",
    "    \n",
    "    maybe_download(url_list,year)\n",
    "    \n",
    "def fetch_year():\n",
    "    \n",
    "     #fetch the year for which the user wants logs\n",
    "    year = input('Enter the year for which you need to fetch the log files: ')\n",
    "    year=int(year)\n",
    "    if(year >= 2003 and year < 2016):\n",
    "        #calling the function to generate dynamic URL\n",
    "        generate_url(year)\n",
    "    else:\n",
    "        print(\"EDGAR log files are available for years 2003-2016. Kindly enter a year within this range\")\n",
    "        fetch_year()\n",
    "\n",
    "fetch_year()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
