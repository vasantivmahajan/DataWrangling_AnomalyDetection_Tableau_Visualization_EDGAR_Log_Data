{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import urllib3 as ur\n",
    "import urllib as ur\n",
    "\n",
    "import os.path\n",
    "import zipfile\n",
    "import tinys3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "Config = configparser.ConfigParser()\n",
    "Config.read('config.ini')\n",
    "Config.sections()\n",
    "def ConfigSectionMap(section):\n",
    "    dict1 = {}\n",
    "    options = Config.options(section)\n",
    "    for option in options:\n",
    "        try:\n",
    "            dict1[option] = Config.get(section, option)\n",
    "            if dict1[option] == -1:\n",
    "                DebugPrint(\"skip: %s\" % option)\n",
    "        except:\n",
    "            print(\"exception on %s!\" % option)\n",
    "            dict1[option] = None\n",
    "    return dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fetch_company_name_cik_table():\n",
    "    CIKs = []\n",
    "    companyNames = []\n",
    "    path = '.'\n",
    "    files = ['cik-list.txt']\n",
    "    for f in files:\n",
    "\n",
    "          with open (f, \"r\") as myfile:\n",
    "            for line in myfile:\n",
    "                #print(line)\n",
    "                values=line.split(':')\n",
    "                companyNames.append(values[len(values)-3])\n",
    "                CIKs.append(values[(len(values)-2)].strip('0'))\n",
    "    df = pd.DataFrame({'CIK': CIKs, 'company': companyNames})\n",
    "    df.to_csv('CIK-mapping.csv')\n",
    "    ###return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the year for which you need to fetch the log files: 90\n",
      "Already present\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'edgardatasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-359a1c6c6496>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0mget_data_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_zip_folder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Part_2_log_datasets\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mget_data_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupload_zip_to_s3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Part_2_log_datasets.zip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-359a1c6c6496>\u001b[0m in \u001b[0;36mupload_zip_to_s3\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[1;31m# Uploading a single file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBUCKET_NAME\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Part_2_log_datasets.zip\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mBUCKET_NAME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'edgardatasets'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python       \n",
    "\n",
    "class GetData:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Retrieves and stores the urllib.urlopen object for a given url\n",
    "        \"\"\"\n",
    "        \n",
    "    def generate_url(self,year):\n",
    "        \n",
    "        #generate the url for fetching the log files for every month's first day\n",
    "        number_of_months=1\n",
    "        while number_of_months < 13:\n",
    "            if(number_of_months <10):\n",
    "                url=\"http://www.sec.gov/dera/data/PublicEDGAR-log-file-data/\"+year+\"/Qtr1/log\"+year+'%02d' % number_of_months+\"01.zip\"\n",
    "            else:\n",
    "                url=\"http://www.sec.gov/dera/data/PublicEDGAR-log-file-data/\"+year+\"/Qtr1/log\"+year+str(number_of_months)+\"01.zip\"\n",
    "            number_of_months=number_of_months+1\n",
    "        #temp_url=download_data(\"http://www.sec.gov/dera/data/Public-EDGAR-log-file-data/2016/Qtr1/log20160101.zip\")\n",
    "        return self.download_data(\"http://www.sec.gov/dera/data/Public-EDGAR-log-file-data/2003/Qtr1/log20030301.zip\")\n",
    "        \n",
    "    def download_data(self,url):\n",
    "\n",
    "        #fetching the zip file name from the URL\n",
    "        file_name=url.split(\"/\")\n",
    "        \n",
    "        self.create_directory(\"Part_2_log_datasets\")\n",
    "\n",
    "        #Downloading data if not already present in the cache\n",
    "        if(os.path.exists(\"Part_2_log_datasets/\"+file_name[8])):\n",
    "            print(\"Already present\")\n",
    "\n",
    "        else:\n",
    "            ur.request.urlretrieve(url, \"Part_2_log_datasets/\"+file_name[8])\n",
    "            print(\"Download complete\")\n",
    "\n",
    "        #unzip the file and fetch the csv file\n",
    "        zf = zipfile.ZipFile(\"Part_2_log_datasets/\"+file_name[8]) \n",
    "        csv_file_name=file_name[8].replace(\"zip\", \"csv\")\n",
    "        zf_file=zf.open(csv_file_name)\n",
    "\n",
    "        #create a dataframe from the csv\n",
    "        df = pd.read_csv(zf_file)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_directory(self,path):\n",
    "        try:\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "        except OSError as exception:\n",
    "            if exception.errno != errno.EEXIST:\n",
    "                raise\n",
    "                \n",
    "    def create_zip_folder(self,path):\n",
    "        zipfolder_name=path+'.zip'\n",
    "        zf = zipfile.ZipFile(zipfolder_name, \"w\")\n",
    "        for dirname, subdirs, files in os.walk(path):\n",
    "            zf.write(dirname)\n",
    "            for filename in files:\n",
    "                zf.write(os.path.join(dirname, filename))\n",
    "        zf.close()\n",
    "    \n",
    "    def upload_zip_to_s3(self,path):\n",
    "        S3_ACCESS_KEY = ConfigSectionMap(\"Part_1\")['s3_access_key']#'AKIAICSMTFLAR54DYMQQ'\n",
    "        S3_SECRET_KEY = ConfigSectionMap(\"Part_1\")['s3_secret_key']#'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'\n",
    "        BUCKET_NAME = ConfigSectionMap(\"Part_1\")['s3_bucket']\n",
    "        #host = ConfigSectionMap(\"Part_1\")['HOST']\n",
    "        # host='edgardatasets.s3-website-us-west-2.amazonaws.com'\n",
    "        # Creating a simple connection\n",
    "        conn = tinys3.Connection(S3_ACCESS_KEY,S3_SECRET_KEY)\n",
    "\n",
    "        # Uploading a single file\n",
    "        f = open(\"Part_2_log_datasets.zip\",'rb')\n",
    "        conn.upload(\"Part_2_log_datasets.zip\",f,BUCKET_NAME)             \n",
    "        \n",
    "#fetch the year for which the user wants logs\n",
    "year = input('Enter the year for which you need to fetch the log files: ')\n",
    "#calling the function to generate dynamic URL\n",
    "get_data_obj=GetData()\n",
    "df=get_data_obj.generate_url(year)\n",
    "        \n",
    "get_data_obj.create_zip_folder(\"Part_2_log_datasets\")\n",
    "get_data_obj.upload_zip_to_s3(\"Part_2_log_datasets.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert all the integer column in int format\n",
    "\n",
    "df['zone'] = df['zone'].astype('int')\n",
    "df['cik'] = df['cik'].astype('int')\n",
    "df['code'] = df['code'].astype('int')\n",
    "df['idx']=df['idx'].astype('int')\n",
    "df['norefer']=df['norefer'].astype('int')\n",
    "df['noagent']=df['noagent'].astype('int')\n",
    "df['find']=df['find'].astype('int')\n",
    "df['crawler']=df['crawler'].astype('int')\n",
    "#print(df.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#replacing empty strings with NaN \n",
    "df.replace(r'\\s+', np.nan, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#replace all ip column NaN value by a default ip address \n",
    "df[\"ip\"].fillna(\"255.255.255.255\", inplace=True)\n",
    "\n",
    "#perform forward fill to replace NaN values by fetching the next valid value\n",
    "df[\"date\"].fillna(method='ffill')\n",
    "\n",
    "#perform backward fill to replace NaN values by backpropagating and fetching the previous valid value\n",
    "df[\"time\"].fillna(method='bfill')\n",
    "\n",
    "#replace all zone column NaN values by 'Not Available' extension\n",
    "df[\"zone\"].fillna(\"Not Available\", inplace=True)\n",
    "\n",
    "#replace all extension column NaN values by default extension\n",
    "df[\"extention\"].fillna(\"-index.htm\", inplace=True)\n",
    "\n",
    "#replace all size column NaN values by 0 and convert the column into integer \n",
    "df[\"size\"].fillna(0, inplace=True)\n",
    "df['size'] = df['size'].astype('int')\n",
    "\n",
    "#replace all user agent column NaN values by the default value 1 (no user agent)\n",
    "df[\"noagent\"].fillna(\"Not Applicable\", inplace=True)\n",
    "\n",
    "#replace all find column NaN values by the default value 0 (no character strings found)\n",
    "df[\"find\"].fillna(0, inplace=True)\n",
    "\n",
    "#replace all broser column NaN values by a string\n",
    "df[\"browser\"].fillna(\"Not Available\", inplace=True)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if the value in idx column is missing, check the value of the extension column, if its \"-index.html\" set the column's value 1 else 0\n",
    "count=0\n",
    "for i in df['idx']:\n",
    "    if(np.isnan(i)):\n",
    "        if(df['extension'][count]==\"-index.htm\"):\n",
    "            i=1\n",
    "        else:\n",
    "            i=0\n",
    "    count=count+1\n",
    "\n",
    "# if the value of norefer column is missing, check the value of the find column, if it is 0, set the value 1, else it set it 0\n",
    "counter=0\n",
    "for i in df['norefer']:\n",
    "    if(np.isnan(i)):\n",
    "        if(df[\"find\"][counter]==0):\n",
    "            i=1\n",
    "        else:\n",
    "            i=0\n",
    "    counter=counter+1\n",
    "    \n",
    "# if the value of crawler is missing, check the value of the code, if it is 404 set it as 1 else 0\n",
    "count_position=0\n",
    "for i in df['crawler']:\n",
    "    if(np.isnan(i)):\n",
    "        if(df[\"code\"][count_position]==404):\n",
    "            i=1\n",
    "        else:\n",
    "            i=0\n",
    "    count_position=count_position+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#insert a column to check CIK, Accession number discripancy\n",
    "df.insert(6, \"CIK_Accession_Anamoly_Flag\", \"N\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#check if CIK and Accession number match. The Accession number is divided into three parts, CIK-Year-Number_of_filings_listed.\n",
    "#the first part i.e the CIK must match with the CIK column. If not, there exists an anomaly\n",
    "\n",
    "count=0;\n",
    "print(\"Creating CIK_Accession_Anomaly_Flag column\")\n",
    "for i in df['accession']:\n",
    "    #fetch the CIK number from the accession number and convert it into integer\n",
    "    list_of_fetched_cik_from_accession=[(int(i.split(\"-\")[0]))]\n",
    "    \n",
    "    #check if the CIK number from the column and CIK number fetched from the accession number are equal\n",
    "    if(df['cik'][count]!=list_of_fetched_cik_from_accession):\n",
    "        df['CIK_Accession_Anamoly_Flag'][count]=\"Y\"\n",
    "        \n",
    "    count=count+1\n",
    "print(\"Done\")\n",
    "#print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#merge both the dataframe using the CIK,cik as common column\n",
    "\n",
    "# read csv from source\n",
    "company_df = pd.read_csv('CIK-mapping.csv') \n",
    "\n",
    "merged_df=pd.merge(company_df, df, left_on='CIK',right_on='cik' )\n",
    "#merged_df.head()\n",
    "#merged_df.loc[merged_df['cik']==1438823]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.insert(7, \"filename\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Extension rule: if the file name is missing and only the file extension is present, then the file name is document accession number\n",
    "count=0\n",
    "for i in df[\"extention\"]:\n",
    "    if(i==\".txt\"):\n",
    "        # if the value in extension is only .txt, fetch the accession number and append accession number to .txt\n",
    "        #list_of_fetched_cik_from_accession=int(((df2[\"accession\"].str.split(\"-\")[count])[0]))\n",
    "        #print((df[\"accession\"]).astype(str))\n",
    "        #list_of_fetched_cik_from_accession=int(df[\"accession\"])\n",
    "        df[\"filename\"][count]=(df[\"accession\"][count])+\".txt\" \n",
    "    else:\n",
    "        df[\"filename\"][count]=i\n",
    "    count=count+1\n",
    "#print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'host': 'edgardatasets.s3-website-us-west-2.amazonaws.com',\n",
       " 's3_access_key': 'AKIAICSMTFLAR54DYMQQ',\n",
       " 's3_secret_key': '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'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConfigSectionMap(\"Part_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
