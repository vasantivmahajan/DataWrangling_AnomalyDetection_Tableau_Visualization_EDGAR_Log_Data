{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import urllib3 as ur\n",
    "import urllib.request as ur\n",
    "import configparser\n",
    "import os.path\n",
    "import zipfile\n",
    "import tinys3\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "import logging as log\n",
    "\n",
    "log.basicConfig(filename='Part_2_log_datasets_trial/EDGAR_LogFileDataset_LogFile.log', level=logging.INFO, format='%(asctime)s %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Config = configparser.ConfigParser()\n",
    "Config.read('config.ini')\n",
    "Config.sections()\n",
    "def ConfigSectionMap(section):\n",
    "    dict1 = {}\n",
    "    options = Config.options(section)\n",
    "    for option in options:\n",
    "        try:\n",
    "            dict1[option] = Config.get(section, option)\n",
    "            if dict1[option] == -1:\n",
    "                DebugPrint(\"skip: %s\" % option)\n",
    "        except:\n",
    "            print(\"exception on %s!\" % option)\n",
    "            dict1[option] = None\n",
    "    return dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the year for which you need to fetch the log files: 2003\n",
      "Generating the URL's for the year\n",
      "Downloading data for all the months\n",
      "Data for  log20030101.zip  is already present, pulling it from cache\n",
      "All the files are downloaded\n",
      "Unzipping the downloaded files\n",
      "Opening the csv files\n",
      "Creating a dataframe from the csv and appending it to the list of dataframe\n",
      "Data for  log20030201.zip  is already present, pulling it from cache\n",
      "All the files are downloaded\n",
      "Unzipping the downloaded files\n",
      "Opening the csv files\n",
      "Creating a dataframe from the csv and appending it to the list of dataframe\n",
      "Data for  log20030301.zip  is already present, pulling it from cache\n",
      "All the files are downloaded\n",
      "Unzipping the downloaded files\n",
      "Opening the csv files\n",
      "Creating a dataframe from the csv and appending it to the list of dataframe\n",
      "Data for  log20030401.zip  is already present, pulling it from cache\n",
      "All the files are downloaded\n",
      "Unzipping the downloaded files\n",
      "Opening the csv files\n",
      "Creating a dataframe from the csv and appending it to the list of dataframe\n",
      "Data for  log20030501.zip  is already present, pulling it from cache\n",
      "All the files are downloaded\n",
      "Unzipping the downloaded files\n",
      "Opening the csv files\n",
      "Creating a dataframe from the csv and appending it to the list of dataframe\n",
      "Data for  log20030601.zip  is already present, pulling it from cache\n",
      "All the files are downloaded\n",
      "Unzipping the downloaded files\n",
      "Opening the csv files\n",
      "Creating a dataframe from the csv and appending it to the list of dataframe\n",
      "Data for  log20030701.zip  is already present, pulling it from cache\n",
      "All the files are downloaded\n",
      "Unzipping the downloaded files\n",
      "Opening the csv files\n",
      "Creating a dataframe from the csv and appending it to the list of dataframe\n",
      "Data for  log20030801.zip  is already present, pulling it from cache\n",
      "All the files are downloaded\n",
      "Unzipping the downloaded files\n",
      "Opening the csv files\n",
      "Creating a dataframe from the csv and appending it to the list of dataframe\n",
      "Data for  log20030901.zip  is already present, pulling it from cache\n",
      "All the files are downloaded\n",
      "Unzipping the downloaded files\n",
      "Opening the csv files\n",
      "Creating a dataframe from the csv and appending it to the list of dataframe\n",
      "Data for  log20031001.zip  is already present, pulling it from cache\n",
      "All the files are downloaded\n",
      "Unzipping the downloaded files\n",
      "Opening the csv files\n",
      "Creating a dataframe from the csv and appending it to the list of dataframe\n",
      "Data for  log20031101.zip  is already present, pulling it from cache\n",
      "All the files are downloaded\n",
      "Unzipping the downloaded files\n",
      "Opening the csv files\n",
      "Creating a dataframe from the csv and appending it to the list of dataframe\n",
      "Data for  log20031201.zip  is already present, pulling it from cache\n",
      "All the files are downloaded\n",
      "Unzipping the downloaded files\n",
      "Opening the csv files\n",
      "Creating a dataframe from the csv and appending it to the list of dataframe\n",
      "Data fetched, started cleaning\n",
      "Formatted the columns of the dataframe\n",
      "Handling missing values completed\n",
      "Exporting merged dataframe to local system\n",
      "Merged dataframe exported\n",
      "Creating CIK_Accession_Anomaly_Flag column to check anomaly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vasanti\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:297: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Vasanti\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:275: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIK Accession Anomaly flag computed\n",
      "                ip        date      time  zone      cik             accession  \\\n",
      "0   129.110.39.jca  2003-03-01  00:00:00   500    97349  0000097349-01-000006   \n",
      "1    61.115.76.jbf  2003-03-01  00:00:00   500   766351  0000950134-03-003149   \n",
      "2    61.115.76.jbf  2003-03-01  00:00:01   500   902584  0000902584-03-000044   \n",
      "3    61.115.76.jbf  2003-03-01  00:00:03   500   778207  9999999997-03-006003   \n",
      "4   129.110.39.jca  2003-03-01  00:00:07   500    97349  0000097349-01-000006   \n",
      "5    208.62.55.eib  2003-03-01  00:00:07   500    56824  0000950124-03-000077   \n",
      "6   129.110.39.jca  2003-03-01  00:00:10   500    97349  0000097349-01-000006   \n",
      "7    208.62.55.eib  2003-03-01  00:00:12   500    56824  0000950124-03-000077   \n",
      "8   129.110.39.jca  2003-03-01  00:00:13   500    97349  0000097349-01-000006   \n",
      "9  148.139.130.hhi  2003-03-01  00:00:16   500  1108205  0000927016-02-005853   \n",
      "\n",
      "  CIK_Accession_Anamoly_Flag          extention  code    size  idx  norefer  \\\n",
      "0                          N          -0002.txt   200    3726    0        0   \n",
      "1                          Y               .txt   200  995957    0        1   \n",
      "2                          N               .txt   200   15520    0        1   \n",
      "3                          Y               .txt   200    1670    0        1   \n",
      "4                          N         -index.htm   200    4331    1        0   \n",
      "5                          Y         -index.htm   200    2727    1        0   \n",
      "6                          N          -0003.txt   200    1211    0        0   \n",
      "7                          Y  k73883e10vqza.txt   200  158260    0        0   \n",
      "8                          N         -index.htm   200    4331    1        0   \n",
      "9                          Y         -index.htm   304       0    1        0   \n",
      "\n",
      "   noagent  find  crawler        browser  \n",
      "0        0     9        0            win  \n",
      "1        0     0        0  Not Available  \n",
      "2        0     0        0  Not Available  \n",
      "3        0     0        0  Not Available  \n",
      "4        0     1        0            win  \n",
      "5        0     1        0            win  \n",
      "6        0     9        0            win  \n",
      "7        0     9        0            win  \n",
      "8        0     1        0            win  \n",
      "9        0     1        0            win  \n",
      "Data zipped and loaded on S3\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python       \n",
    "merged_dataframe=pd.DataFrame()\n",
    "df_list_global=list()\n",
    "class GetData:\n",
    "   \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Retrieves and stores the urllib.urlopen object for a given url\n",
    "        \"\"\"\n",
    "    def create_directory(self,path):\n",
    "        try:\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "        except OSError as exception:\n",
    "            if exception.errno != errno.EEXIST:\n",
    "                raise\n",
    "    \n",
    "    def setDataFrame(self, df):\n",
    "        merged_dataframe = df\n",
    "        \n",
    "    def getDataFrame(self):\n",
    "        return merged_dataframe\n",
    "    \n",
    "    def setDataFrameList(self, list_of_df):\n",
    "        \n",
    "        df_list_global = list_of_df\n",
    "      \n",
    "        \n",
    "    def getDataFrameList(self):\n",
    "        return df_list_global\n",
    "    \n",
    "    def maybe_download(self, url_list, year):\n",
    "        \n",
    "        df_list=['df1','df2','df3','df4','df5','df6','df7','df8','df9','df10','df11','df12']\n",
    "        year=str(year)\n",
    "        count=0\n",
    "        print(\"Downloading data for all the months\")\n",
    "        log.info(\"Downloading data for all the months\")\n",
    "        \n",
    "        for i in url_list:\n",
    "\n",
    "            #fetching the zip file name from the URL\n",
    "            file_name=i.split(\"/\")\n",
    "            self.create_directory(\"Part_2_log_datasets_trial/\"+year+\"/\")\n",
    "\n",
    "            #Downloading data if not already present in the cache\n",
    "            if(os.path.exists(\"Part_2_log_datasets_trial/\"+year+\"/\"+file_name[8])):\n",
    "                print(\"Data for \",file_name[8],\" is already present, pulling it from cache\")\n",
    "                \n",
    "\n",
    "            else:\n",
    "                #pbar = ProgressBar(widgets=[Percentage(), Bar()])\n",
    "                ur.urlretrieve(i, \"Part_2_log_datasets_trial/\"+year+\"/\"+file_name[8])\n",
    "                #ur.urlretrieve(i, \"Part_2_log_datasets_trial/\"+year+\"/\"+file_name[8], reporthook)\n",
    "                print(\"Data for \",file_name[8],\"not present in cache. Downloading data\")\n",
    "                \n",
    "            \n",
    "            #unzip the file and fetch the csv file\n",
    "            zf = zipfile.ZipFile(\"Part_2_log_datasets_trial/\"+year+\"/\"+file_name[8]) \n",
    "            csv_file_name=file_name[8].replace(\"zip\", \"csv\")\n",
    "            zf_file=zf.open(csv_file_name)\n",
    "            \n",
    "            \n",
    "            #create a dataframe from the csv and append it to the list of dataframe\n",
    "            df_list[count]=pd.read_csv(zf_file)\n",
    "           \n",
    "            count=count+1 \n",
    "        \n",
    "        print(\"All the files are downloaded and unzipped\")\n",
    "        log.info(\"All the files are downloaded and unzipped\")\n",
    "        print(\"Creating a dataframe from the csv and appending it to the list of dataframe\")\n",
    "        log.info(\"Creating a dataframe from the csv and appending it to the list of dataframe\")\n",
    "        \n",
    "        self.setDataFrameList(df_list)\n",
    "        log.info(\"Merging the dataframe\")\n",
    "        #merging the data into one dataframe\n",
    "        merged_dataframe=pd.concat([df_list[0],df_list[1],df_list[2],df_list[3],df_list[4],df_list[5],df_list[6],df_list[7],df_list[8],df_list[9],df_list[10],df_list[11]], ignore_index=True)\n",
    "        self.setDataFrame(merged_dataframe)\n",
    "        return merged_dataframe\n",
    "    \n",
    "\n",
    "    def generate_url(self, year):\n",
    "        log.info('In generate URL method')\n",
    "        print(\"Generating the URL's for the year\")\n",
    "        log.info(\"Generating the URL's for the year\")\n",
    "        url_list=list()\n",
    "        #generate the url for fetching the log files for every month's first day\n",
    "        number_of_months=1\n",
    "\n",
    "        while number_of_months < 13:\n",
    "            #find the quarter for the month\n",
    "            if number_of_months >= 1 and number_of_months < 4:\n",
    "                quarter=\"Qtr1\"\n",
    "            elif(number_of_months >= 4 and number_of_months < 7):\n",
    "                quarter=\"Qtr2\"\n",
    "            elif(number_of_months >= 7 and number_of_months < 10):\n",
    "                quarter=\"Qtr3\"\n",
    "            elif(number_of_months >= 10 and number_of_months < 13):\n",
    "                quarter=\"Qtr4\"\n",
    "\n",
    "            if(number_of_months <10):\n",
    "                url=\"http://www.sec.gov/dera/data/Public-EDGAR-log-file-data/\"+str(year)+\"/\"+quarter+\"/log\"+str(year)+'%02d' % number_of_months+\"01.zip\"\n",
    "\n",
    "            else:\n",
    "                url=\"http://www.sec.gov/dera/data/Public-EDGAR-log-file-data/\"+str(year)+\"/\"+quarter+\"/log\"+str(year)+str(number_of_months)+\"01.zip\"\n",
    "            \n",
    "            \n",
    "            url_list.append(url)\n",
    "            number_of_months=number_of_months+1\n",
    "\n",
    "        return self.maybe_download(url_list,year)\n",
    "        \n",
    "    def fetch_year(self):\n",
    "        log.info('Start of program')\n",
    "         #fetch the year for which the user wants logs\n",
    "        year = input('Enter the year for which you need to fetch the log files: ')\n",
    "        \n",
    "        year=int(year)\n",
    "        if(year >= 2003 and year < 2016):\n",
    "            #calling the function to generate dynamic URL\n",
    "            return self.generate_url(year)\n",
    "        else:\n",
    "            print(\"EDGAR log files are available for years 2003-2016. Kindly enter a year within this range\")\n",
    "            fetch_year()\n",
    "    \n",
    "                \n",
    "    def create_zip_folder(self,path):\n",
    "        zipfolder_name=path+'.zip'\n",
    "        zf = zipfile.ZipFile(zipfolder_name, \"w\")\n",
    "        for dirname, subdirs, files in os.walk(path):\n",
    "            zf.write(dirname)\n",
    "            for filename in files:\n",
    "                zf.write(os.path.join(dirname, filename))\n",
    "        zf.close()\n",
    "    \n",
    "    def upload_zip_to_s3(self,path):\n",
    "        S3_ACCESS_KEY = ConfigSectionMap(\"Part_1\")['s3_access_key']#'AKIAICSMTFLAR54DYMQQ'\n",
    "        S3_SECRET_KEY = ConfigSectionMap(\"Part_1\")['s3_secret_key']#'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'\n",
    "        BUCKET_NAME = ConfigSectionMap(\"Part_1\")['s3_bucket']\n",
    "        #host = ConfigSectionMap(\"Part_1\")['HOST']\n",
    "        # host='edgardatasets.s3-website-us-west-2.amazonaws.com'\n",
    "        # Creating a simple connection\n",
    "        conn = tinys3.Connection(S3_ACCESS_KEY,S3_SECRET_KEY)\n",
    "\n",
    "        # Uploading a single file\n",
    "        f = open(\"Part_2_log_datasets_trial.zip\",'rb')\n",
    "        conn.upload(\"Part_2_log_datasets_trial.zip\",f,BUCKET_NAME)  \n",
    "        \n",
    "get_data_obj=GetData()\n",
    "merged_dataframe=get_data_obj.fetch_year()\n",
    "#fetch the year for which the user wants logs\n",
    "#year = input('Enter the year for which you need to fetch the log files: ')\n",
    "#calling the function to generate dynamic URL\n",
    "\n",
    "#df=get_data_obj.generate_url(year)\n",
    "\n",
    "\n",
    "class Process_and_analyse_data():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Retrieves and stores the urllib.urlopen object for a given url\n",
    "        \"\"\"\n",
    "    \n",
    "    def format_dataframe_columns(self):\n",
    "        #convert all the integer column in int format\n",
    "        log.info(\"Data fetched, started cleaning\")\n",
    "        print(\"Data fetched, started cleaning\")\n",
    "        df['zone'] = df['zone'].astype('int')\n",
    "        df['cik'] = df['cik'].astype('int')\n",
    "        df['code'] = df['code'].astype('int')\n",
    "        df['idx']=df['idx'].astype('int')\n",
    "        df['norefer']=df['norefer'].astype('int')\n",
    "        df['noagent']=df['noagent'].astype('int')\n",
    "        df['find']=df['find'].astype('int')\n",
    "        df['crawler']=df['crawler'].astype('int')\n",
    "        \n",
    "        #replacing empty strings with NaN \n",
    "        df.replace(r'\\s+', np.nan, regex=True)\n",
    "        log.info(\"Formatted the columns of the dataframe\")\n",
    "        print(\"Formatted the columns of the dataframe\")\n",
    "        self.handle_nan_values()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def handle_nan_values(self):\n",
    "        \n",
    "        #replace all ip column NaN value by a default ip address \n",
    "        df[\"ip\"].fillna(\"255.255.255.255\", inplace=True)\n",
    "\n",
    "        #perform forward fill to replace NaN values by fetching the next valid value\n",
    "        df[\"date\"].fillna(method='ffill')\n",
    "\n",
    "        #perform backward fill to replace NaN values by backpropagating and fetching the previous valid value\n",
    "        df[\"time\"].fillna(method='bfill')\n",
    "\n",
    "        #replace all zone column NaN values by 'Not Available' extension\n",
    "        df[\"zone\"].fillna(\"Not Available\", inplace=True)\n",
    "\n",
    "        #replace all extension column NaN values by default extension\n",
    "        df[\"extention\"].fillna(\"-index.htm\", inplace=True)\n",
    "\n",
    "        #replace all size column NaN values by 0 and convert the column into integer \n",
    "        df[\"size\"].fillna(0, inplace=True)\n",
    "        df['size'] = df['size'].astype('int')\n",
    "\n",
    "        #replace all user agent column NaN values by the default value 1 (no user agent)\n",
    "        df[\"noagent\"].fillna(\"Not Applicable\", inplace=True)\n",
    "\n",
    "        #replace all find column NaN values by the default value 0 (no character strings found)\n",
    "        df[\"find\"].fillna(0, inplace=True)\n",
    "\n",
    "        #replace all broser column NaN values by a string\n",
    "        df[\"browser\"].fillna(\"Not Available\", inplace=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # if the value in idx column is missing, check the value of the extension column, if its \"-index.html\" set the column's value 1 else 0\n",
    "        count=0\n",
    "        for i in df['idx']:\n",
    "            if(np.isnan(i)):\n",
    "                if(df['extension'][count]==\"-index.htm\"):\n",
    "                    i=1\n",
    "                else:\n",
    "                    i=0\n",
    "            count=count+1\n",
    "\n",
    "        # if the value of norefer column is missing, check the value of the find column, if it is 0, set the value 1, else it set it 0\n",
    "        counter=0\n",
    "        for i in df['norefer']:\n",
    "            if(np.isnan(i)):\n",
    "                if(df[\"find\"][counter]==0):\n",
    "                    i=1\n",
    "                else:\n",
    "                    i=0\n",
    "            counter=counter+1\n",
    "\n",
    "        # if the value of crawler is missing, check the value of the code, if it is 404 set it as 1 else 0\n",
    "        count_position=0\n",
    "        for i in df['crawler']:\n",
    "            if(np.isnan(i)):\n",
    "                if(df[\"code\"][count_position]==404):\n",
    "                    i=1\n",
    "                else:\n",
    "                    i=0\n",
    "            count_position=count_position+1\n",
    "        log.info(\"Replacing NaN values with appropriate replacement\")\n",
    "        log.info(\"Handling missing values completed\")\n",
    "        print(\"Handling missing values completed\")\n",
    "        \n",
    "        log.info(\"Exporting merged dataframe to local system\")\n",
    "        print(\"Exporting merged dataframe to local system\")\n",
    "        df.to_csv(\"Part_2_log_datasets_trial/merged_dataframe.csv\")\n",
    "        log.info(\"Merged dataframe exported\")\n",
    "        print(\"Merged dataframe exported\")\n",
    "        self.fetch_company_name_from_cik()\n",
    "    \n",
    "    def fetch_company_name_from_cik(self):\n",
    "        #we found a list of CIK their company names from a EDGAR's github repository. We are fetching this information to gain the information about company name\n",
    "        company_df = pd.read_csv('CIK-mapping.csv') \n",
    "        #renaming column, so that both the dataframes can be merged on the common column\n",
    "        company_df = company_df.rename(columns={'CIK': 'cik'})\n",
    "        company_df['cik'] = company_df['cik'].astype(np.int64)\n",
    "        #merging both the dataframes\n",
    "        #merged_df_cik_company_name= df.join(company_df, on='cik', how='left', rsuffix=\"_review\")\n",
    "        merged_df_cik_company_name=pd.merge(df, company_df, on='cik')\n",
    "        #merged_df_cik_company_name=pd.merge(df,company_df, left_on='cik',right_on='CIK' )\n",
    "        #print(merged_df_cik_company_name.head(10))\n",
    "        \n",
    "        self.identify_cik_accession_number_anomaly()\n",
    "        \n",
    "        \n",
    "    def identify_cik_accession_number_anomaly(self):\n",
    "        #this operation requires a large amount of time for computaton, thus we are performing this on a subset of data\n",
    "        small_df=df.head(25)\n",
    "        #insert a column to check CIK, Accession number discripancy\n",
    "        small_df.insert(6, \"CIK_Accession_Anamoly_Flag\", \"N\")\n",
    "                \n",
    "        #check if CIK and Accession number match. The Accession number is divided into three parts, CIK-Year-Number_of_filings_listed.\n",
    "        #the first part i.e the CIK must match with the CIK column. If not, there exists an anomaly\n",
    "\n",
    "        count=0;\n",
    "        print(\"Creating CIK_Accession_Anomaly_Flag column to check anomaly\")\n",
    "        log.info(\"Creating CIK_Accession_Anomaly_Flag column to check anomaly\")\n",
    "        \n",
    "        for i in small_df['accession']:\n",
    "            #fetch the CIK number from the accession number and convert it into integer\n",
    "            list_of_fetched_cik_from_accession=[(int(i.split(\"-\")[0]))]\n",
    "\n",
    "            #check if the CIK number from the column and CIK number fetched from the accession number are equal\n",
    "            if(small_df['cik'][count]!=list_of_fetched_cik_from_accession):\n",
    "                small_df['CIK_Accession_Anamoly_Flag'][count]=\"Y\"\n",
    "\n",
    "            count=count+1\n",
    "        log.info(\"CIK Accession Anomaly flag computed\")\n",
    "        print(\"CIK Accession Anomaly flag computed\")\n",
    "        self.get_file_name_from_extension()\n",
    "        \n",
    "    def get_file_name_from_extension():\n",
    "        #this operation requires a large amount of time for computaton, thus we are performing this on a subset of data\n",
    "        small_df=df.head(25)\n",
    "        small_df.insert(7, \"filename\", \"\")\n",
    "        print(\"Creating filename column\")\n",
    "        log.info(\"Creating filename column\")\n",
    "        #Extension rule: if the file name is missing and only the file extension is present, then the file name is document accession number\n",
    "        count=0\n",
    "        for i in small_df[\"extention\"]:\n",
    "            if(i==\".txt\"):\n",
    "                # if the value in extension is only .txt, fetch the accession number and append accession number to .txt\n",
    "                #list_of_fetched_cik_from_accession=int(((df2[\"accession\"].str.split(\"-\")[count])[0]))\n",
    "                #print((df[\"accession\"]).astype(str))\n",
    "                #list_of_fetched_cik_from_accession=int(df[\"accession\"])\n",
    "                small_df[\"filename\"][count]=(small_df[\"accession\"][count])+\".txt\" \n",
    "            else:\n",
    "                small_df[\"filename\"][count]=i\n",
    "            count=count+1\n",
    "        print(\"Filename column created\")\n",
    "        log.info(\"Filename column created\")\n",
    "        \n",
    "get_data_obj=GetData()\n",
    "df=get_data_obj.getDataFrame()\n",
    "df_list=get_data_obj.getDataFrameList()\n",
    "process_data_obj=Process_and_analyse_data()\n",
    "process_data_obj.format_dataframe_columns()\n",
    "\n",
    "log.info(\"Zipping the folder for loading in S3\")\n",
    "get_data_obj.create_zip_folder(\"Part_2_log_datasets_trial\")\n",
    "get_data_obj.upload_zip_to_s3(\"Part_2_log_datasets_trial.zip\")\n",
    "log.info(\"Data zipped and loaded on S3\")\n",
    "print(\"Data zipped and loaded on S3\")\n",
    "log.info(\"Pipeline completed!!\")\n",
    "log.info(\"<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
