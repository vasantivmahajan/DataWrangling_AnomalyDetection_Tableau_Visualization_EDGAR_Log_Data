{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import urllib3 as ur\n",
    "import urllib as ur\n",
    "\n",
    "import os.path\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fetch_company_name_cik_table():\n",
    "    CIKs = []\n",
    "    companyNames = []\n",
    "    path = '.'\n",
    "    files = ['cik-list.txt']\n",
    "    for f in files:\n",
    "\n",
    "          with open (f, \"r\") as myfile:\n",
    "            for line in myfile:\n",
    "                #print(line)\n",
    "                values=line.split(':')\n",
    "                companyNames.append(values[len(values)-3])\n",
    "                CIKs.append(values[(len(values)-2)].strip('0'))\n",
    "    df = pd.DataFrame({'CIK': CIKs, 'company': companyNames})\n",
    "    df.to_csv('CIK-mapping.csv')\n",
    "    ###return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python       \n",
    "\n",
    "class GetData:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Retrieves and stores the urllib.urlopen object for a given url\n",
    "        \"\"\"\n",
    "        \n",
    "    def generate_url(self,year):\n",
    "        \n",
    "        #generate the url for fetching the log files for every month's first day\n",
    "        number_of_months=1\n",
    "        while number_of_months < 13:\n",
    "            if(number_of_months <10):\n",
    "                url=\"http://www.sec.gov/dera/data/PublicEDGAR-log-file-data/\"+year+\"/Qtr1/log\"+year+'%02d' % number_of_months+\"01.zip\"\n",
    "            else:\n",
    "                url=\"http://www.sec.gov/dera/data/PublicEDGAR-log-file-data/\"+year+\"/Qtr1/log\"+year+str(number_of_months)+\"01.zip\"\n",
    "            number_of_months=number_of_months+1\n",
    "        #temp_url=download_data(\"http://www.sec.gov/dera/data/Public-EDGAR-log-file-data/2016/Qtr1/log20160101.zip\")\n",
    "        return self.download_data(\"http://www.sec.gov/dera/data/Public-EDGAR-log-file-data/2003/Qtr1/log20030301.zip\")\n",
    "        \n",
    "    def download_data(self,url):\n",
    "\n",
    "        #fetching the zip file name from the URL\n",
    "        file_name=url.split(\"/\")\n",
    "        \n",
    "        self.create_directory(\"Part_2_log_datasets\")\n",
    "\n",
    "        #Downloading data if not already present in the cache\n",
    "        if(os.path.exists(\"Part_2_log_datasets/\"+file_name[8])):\n",
    "            print(\"Already present\")\n",
    "\n",
    "        else:\n",
    "            ur.request.urlretrieve(url, \"Part_2_log_datasets/\"+file_name[8])\n",
    "            print(\"Download complete\")\n",
    "\n",
    "        #unzip the file and fetch the csv file\n",
    "        zf = zipfile.ZipFile(\"Part_2_log_datasets/\"+file_name[8]) \n",
    "        csv_file_name=file_name[8].replace(\"zip\", \"csv\")\n",
    "        zf_file=zf.open(csv_file_name)\n",
    "\n",
    "        #create a dataframe from the csv\n",
    "        df = pd.read_csv(zf_file)\n",
    "        return df\n",
    "    \n",
    "    def create_directory(self,path):\n",
    "        try:\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "        except OSError as exception:\n",
    "            if exception.errno != errno.EEXIST:\n",
    "                raise    \n",
    "        \n",
    "#fetch the year for which the user wants logs\n",
    "year = input('Enter the year for which you need to fetch the log files: ')\n",
    "#calling the function to generate dynamic URL\n",
    "get_data_obj=GetData()\n",
    "df=get_data_obj.generate_url(year)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert all the integer column in int format\n",
    "\n",
    "df['zone'] = df['zone'].astype('int')\n",
    "df['cik'] = df['cik'].astype('int')\n",
    "df['code'] = df['code'].astype('int')\n",
    "df['idx']=df['idx'].astype('int')\n",
    "df['norefer']=df['norefer'].astype('int')\n",
    "df['noagent']=df['noagent'].astype('int')\n",
    "df['find']=df['find'].astype('int')\n",
    "df['crawler']=df['crawler'].astype('int')\n",
    "print(df.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#replacing empty strings with NaN \n",
    "df.replace(r'\\s+', np.nan, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#replace all ip column NaN value by a default ip address \n",
    "df[\"ip\"].fillna(\"255.255.255.255\", inplace=True)\n",
    "\n",
    "#perform forward fill to replace NaN values by fetching the next valid value\n",
    "df[\"date\"].fillna(method='ffill')\n",
    "\n",
    "#perform backward fill to replace NaN values by backpropagating and fetching the previous valid value\n",
    "df[\"time\"].fillna(method='bfill')\n",
    "\n",
    "#replace all zone column NaN values by 'Not Available' extension\n",
    "df[\"zone\"].fillna(\"Not Available\", inplace=True)\n",
    "\n",
    "#replace all extension column NaN values by default extension\n",
    "df[\"extention\"].fillna(\"-index.htm\", inplace=True)\n",
    "\n",
    "#replace all size column NaN values by 0 and convert the column into integer \n",
    "df[\"size\"].fillna(0, inplace=True)\n",
    "df['size'] = df['size'].astype('int')\n",
    "\n",
    "#replace all user agent column NaN values by the default value 1 (no user agent)\n",
    "df[\"noagent\"].fillna(\"Not Applicable\", inplace=True)\n",
    "\n",
    "#replace all find column NaN values by the default value 0 (no character strings found)\n",
    "df[\"find\"].fillna(0, inplace=True)\n",
    "\n",
    "#replace all broser column NaN values by a string\n",
    "df[\"browser\"].fillna(\"Not Available\", inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if the value in idx column is missing, check the value of the extension column, if its \"-index.html\" set the column's value 1 else 0\n",
    "count=0\n",
    "for i in df['idx']:\n",
    "    if(np.isnan(i)):\n",
    "        if(df['extension'][count]==\"-index.htm\"):\n",
    "            i=1\n",
    "        else:\n",
    "            i=0\n",
    "    count=count+1\n",
    "\n",
    "# if the value of norefer column is missing, check the value of the find column, if it is 0, set the value 1, else it set it 0\n",
    "counter=0\n",
    "for i in df['norefer']:\n",
    "    if(np.isnan(i)):\n",
    "        if(df[\"find\"][counter]==0):\n",
    "            i=1\n",
    "        else:\n",
    "            i=0\n",
    "    counter=counter+1\n",
    "    \n",
    "# if the value of crawler is missing, check the value of the code, if it is 404 set it as 1 else 0\n",
    "count_position=0\n",
    "for i in df['crawler']:\n",
    "    if(np.isnan(i)):\n",
    "        if(df[\"code\"][count_position]==404):\n",
    "            i=1\n",
    "        else:\n",
    "            i=0\n",
    "    count_position=count_position+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#insert a column to check CIK, Accession number discripancy\n",
    "df.insert(6, \"CIK_Accession_Anamoly_Flag\", \"N\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#check if CIK and Accession number match. The Accession number is divided into three parts, CIK-Year-Number_of_filings_listed.\n",
    "#the first part i.e the CIK must match with the CIK column. If not, there exists an anomaly\n",
    "\n",
    "count=0;\n",
    "print(\"I am working\")\n",
    "for i in df['accession']:\n",
    "    #fetch the CIK number from the accession number and convert it into integer\n",
    "    list_of_fetched_cik_from_accession=[(int(i.split(\"-\")[0]))]\n",
    "    \n",
    "    #check if the CIK number from the column and CIK number fetched from the accession number are equal\n",
    "    if(df['cik'][count]!=list_of_fetched_cik_from_accession):\n",
    "        df['CIK_Accession_Anamoly_Flag'][count]=\"Y\"\n",
    "        \n",
    "    count=count+1\n",
    "print(\"Done\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#merge both the dataframe using the CIK,cik as common column\n",
    "\n",
    "# read csv from source\n",
    "company_df = pd.read_csv('CIK-mapping.csv') \n",
    "\n",
    "merged_df=pd.merge(company_df, df, left_on='CIK',right_on='cik' )\n",
    "merged_df.head()\n",
    "#merged_df.loc[merged_df['cik']==1438823]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.insert(7, \"filename\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Extension rule: if the file name is missing and only the file extension is present, then the file name is document accession number\n",
    "count=0\n",
    "for i in df[\"extention\"]:\n",
    "    if(i==\".txt\"):\n",
    "        # if the value in extension is only .txt, fetch the accession number and append accession number to .txt\n",
    "        #list_of_fetched_cik_from_accession=int(((df2[\"accession\"].str.split(\"-\")[count])[0]))\n",
    "        #print((df[\"accession\"]).astype(str))\n",
    "        #list_of_fetched_cik_from_accession=int(df[\"accession\"])\n",
    "        df[\"filename\"][count]=(df[\"accession\"][count]).astype(str)+\".txt\"\n",
    "    else:\n",
    "        df[\"filename\"][count]=i\n",
    "    count=count+1\n",
    "print(df.head(10))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
