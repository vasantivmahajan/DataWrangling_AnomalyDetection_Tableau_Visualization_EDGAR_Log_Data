{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import urllib3 as ur\n",
    "import urllib as ur\n",
    "import configparser\n",
    "import os.path\n",
    "import zipfile\n",
    "import tinys3\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "Config = configparser.ConfigParser()\n",
    "Config.read('config.ini')\n",
    "Config.sections()\n",
    "def ConfigSectionMap(section):\n",
    "    dict1 = {}\n",
    "    options = Config.options(section)\n",
    "    for option in options:\n",
    "        try:\n",
    "            dict1[option] = Config.get(section, option)\n",
    "            if dict1[option] == -1:\n",
    "                DebugPrint(\"skip: %s\" % option)\n",
    "        except:\n",
    "            print(\"exception on %s!\" % option)\n",
    "            dict1[option] = None\n",
    "    return dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fetch_company_name_cik_table():\n",
    "    CIKs = []\n",
    "    companyNames = []\n",
    "    path = '.'\n",
    "    files = ['cik-list.txt']\n",
    "    for f in files:\n",
    "\n",
    "          with open (f, \"r\") as myfile:\n",
    "            for line in myfile:\n",
    "                #print(line)\n",
    "                values=line.split(':')\n",
    "                companyNames.append(values[len(values)-3])\n",
    "                CIKs.append(values[(len(values)-2)].strip('0'))\n",
    "    df = pd.DataFrame({'CIK': CIKs, 'company': companyNames})\n",
    "    df.to_csv('CIK-mapping.csv')\n",
    "    ###return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the year for which you need to fetch the log files: 2003\n",
      "Data for  log20030101.zip  is already present, pulling it from cache\n",
      "Data for  log20030201.zip  is already present, pulling it from cache\n",
      "Data for  log20030301.zip  is already present, pulling it from cache\n",
      "Data for  log20030401.zip  is already present, pulling it from cache\n",
      "Data for  log20030501.zip  is already present, pulling it from cache\n",
      "Data for  log20030601.zip  is already present, pulling it from cache\n",
      "Data for  log20030701.zip  is already present, pulling it from cache\n",
      "Data for  log20030801.zip  is already present, pulling it from cache\n",
      "Data for  log20030901.zip  is already present, pulling it from cache\n",
      "Data for  log20031001.zip  is already present, pulling it from cache\n",
      "Data for  log20031101.zip  is already present, pulling it from cache\n",
      "Data for  log20031201.zip  is already present, pulling it from cache\n",
      "                ip        date      time  zone      cik             accession  \\\n",
      "0   129.110.39.jca  2003-03-01  00:00:00   500    97349  0000097349-01-000006   \n",
      "1    61.115.76.jbf  2003-03-01  00:00:00   500   766351  0000950134-03-003149   \n",
      "2    61.115.76.jbf  2003-03-01  00:00:01   500   902584  0000902584-03-000044   \n",
      "3    61.115.76.jbf  2003-03-01  00:00:03   500   778207  9999999997-03-006003   \n",
      "4   129.110.39.jca  2003-03-01  00:00:07   500    97349  0000097349-01-000006   \n",
      "5    208.62.55.eib  2003-03-01  00:00:07   500    56824  0000950124-03-000077   \n",
      "6   129.110.39.jca  2003-03-01  00:00:10   500    97349  0000097349-01-000006   \n",
      "7    208.62.55.eib  2003-03-01  00:00:12   500    56824  0000950124-03-000077   \n",
      "8   129.110.39.jca  2003-03-01  00:00:13   500    97349  0000097349-01-000006   \n",
      "9  148.139.130.hhi  2003-03-01  00:00:16   500  1108205  0000927016-02-005853   \n",
      "\n",
      "           extention  code    size  idx  norefer  noagent  find  crawler  \\\n",
      "0          -0002.txt   200    3726    0        0        0     9        0   \n",
      "1               .txt   200  995957    0        1        0     0        0   \n",
      "2               .txt   200   15520    0        1        0     0        0   \n",
      "3               .txt   200    1670    0        1        0     0        0   \n",
      "4         -index.htm   200    4331    1        0        0     1        0   \n",
      "5         -index.htm   200    2727    1        0        0     1        0   \n",
      "6          -0003.txt   200    1211    0        0        0     9        0   \n",
      "7  k73883e10vqza.txt   200  158260    0        0        0     9        0   \n",
      "8         -index.htm   200    4331    1        0        0     1        0   \n",
      "9         -index.htm   304       0    1        0        0     1        0   \n",
      "\n",
      "         browser  Unnamed: 0  cik_review                          company  \n",
      "0            win     97349.0   1409446.0  CAPITAL RESOURCE ALLIANCE, INC.  \n",
      "1  Not Available         NaN         NaN                              NaN  \n",
      "2  Not Available         NaN         NaN                              NaN  \n",
      "3  Not Available         NaN         NaN                              NaN  \n",
      "4            win     97349.0   1409446.0  CAPITAL RESOURCE ALLIANCE, INC.  \n",
      "5            win     56824.0     13856.0                BAYER PAUL EUGENE  \n",
      "6            win     97349.0   1409446.0  CAPITAL RESOURCE ALLIANCE, INC.  \n",
      "7            win     56824.0     13856.0                BAYER PAUL EUGENE  \n",
      "8            win     97349.0   1409446.0  CAPITAL RESOURCE ALLIANCE, INC.  \n",
      "9            win         NaN         NaN                              NaN  \n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python       \n",
    "merged_dataframe=pd.DataFrame()\n",
    "class GetData:\n",
    "   \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Retrieves and stores the urllib.urlopen object for a given url\n",
    "        \"\"\"\n",
    "    def create_directory(self,path):\n",
    "        try:\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "        except OSError as exception:\n",
    "            if exception.errno != errno.EEXIST:\n",
    "                raise\n",
    "    \n",
    "    def setDataFrame(self, df):\n",
    "        merged_dataframe = df\n",
    "        \n",
    "    def getDataFrame(self):\n",
    "        return merged_dataframe\n",
    "    \n",
    "    def maybe_download(self, url_list, year):\n",
    "   \n",
    "        list=['df1','df2','df3','df4','df5','df6','df7','df8','df9','df10','df11','df12']\n",
    "        year=str(year)\n",
    "        count=0\n",
    "        for i in url_list:\n",
    "\n",
    "            #fetching the zip file name from the URL\n",
    "            file_name=i.split(\"/\")\n",
    "            self.create_directory(\"Part_2_log_datasets_trial/\"+year+\"/\")\n",
    "\n",
    "            #Downloading data if not already present in the cache\n",
    "            if(os.path.exists(\"Part_2_log_datasets_trial/\"+year+\"/\"+file_name[8])):\n",
    "                print(\"Data for \",file_name[8],\" is already present, pulling it from cache\")\n",
    "\n",
    "            else:\n",
    "                #pbar = ProgressBar(widgets=[Percentage(), Bar()])\n",
    "\n",
    "\n",
    "                urllib.request.urlretrieve(i, \"Part_2_log_datasets_trial/\"+year+\"/\"+file_name[8], reporthook)\n",
    "                print(\"Data for \",file_name[8],\"not present in cache. Downloading data\")\n",
    "\n",
    "            #unzip the file and fetch the csv file\n",
    "            zf = zipfile.ZipFile(\"Part_2_log_datasets_trial/\"+year+\"/\"+file_name[8]) \n",
    "            csv_file_name=file_name[8].replace(\"zip\", \"csv\")\n",
    "            zf_file=zf.open(csv_file_name)\n",
    "\n",
    "            #create a dataframe from the csv and append it to the list of dataframe\n",
    "            list[count]=pd.read_csv(zf_file)\n",
    "            count=count+1  \n",
    "        #merging the data into one dataframe\n",
    "        merged_dataframe=pd.concat([list[0],list[1],list[2],list[3],list[4],list[5],list[6],list[7],list[8],list[9],list[10],list[11]], ignore_index=True)\n",
    "        self.setDataFrame(merged_dataframe)\n",
    "        return merged_dataframe\n",
    "    \n",
    "\n",
    "    def generate_url(self, year):\n",
    "        url_list=list()\n",
    "        #generate the url for fetching the log files for every month's first day\n",
    "        number_of_months=1\n",
    "\n",
    "        while number_of_months < 13:\n",
    "            #find the quarter for the month\n",
    "            if number_of_months >= 1 and number_of_months < 4:\n",
    "                quarter=\"Qtr1\"\n",
    "            elif(number_of_months >= 4 and number_of_months < 7):\n",
    "                quarter=\"Qtr2\"\n",
    "            elif(number_of_months >= 7 and number_of_months < 10):\n",
    "                quarter=\"Qtr3\"\n",
    "            elif(number_of_months >= 10 and number_of_months < 13):\n",
    "                quarter=\"Qtr4\"\n",
    "\n",
    "            if(number_of_months <10):\n",
    "                url=\"http://www.sec.gov/dera/data/Public-EDGAR-log-file-data/\"+str(year)+\"/\"+quarter+\"/log\"+str(year)+'%02d' % number_of_months+\"01.zip\"\n",
    "\n",
    "            else:\n",
    "                url=\"http://www.sec.gov/dera/data/Public-EDGAR-log-file-data/\"+str(year)+\"/\"+quarter+\"/log\"+str(year)+str(number_of_months)+\"01.zip\"\n",
    "\n",
    "            url_list.append(url)\n",
    "            number_of_months=number_of_months+1\n",
    "\n",
    "        return self.maybe_download(url_list,year)\n",
    "        \n",
    "    def fetch_year(self):\n",
    "    \n",
    "         #fetch the year for which the user wants logs\n",
    "        year = input('Enter the year for which you need to fetch the log files: ')\n",
    "        year=int(year)\n",
    "        if(year >= 2003 and year < 2016):\n",
    "            #calling the function to generate dynamic URL\n",
    "            return self.generate_url(year)\n",
    "        else:\n",
    "            print(\"EDGAR log files are available for years 2003-2016. Kindly enter a year within this range\")\n",
    "            fetch_year()\n",
    "    \n",
    "                \n",
    "    def create_zip_folder(self,path):\n",
    "        zipfolder_name=path+'.zip'\n",
    "        zf = zipfile.ZipFile(zipfolder_name, \"w\")\n",
    "        for dirname, subdirs, files in os.walk(path):\n",
    "            zf.write(dirname)\n",
    "            for filename in files:\n",
    "                zf.write(os.path.join(dirname, filename))\n",
    "        zf.close()\n",
    "    \n",
    "    def upload_zip_to_s3(self,path):\n",
    "        S3_ACCESS_KEY = ConfigSectionMap(\"Part_1\")['s3_access_key']#'AKIAICSMTFLAR54DYMQQ'\n",
    "        S3_SECRET_KEY = ConfigSectionMap(\"Part_1\")['s3_secret_key']#'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'\n",
    "        BUCKET_NAME = ConfigSectionMap(\"Part_1\")['s3_bucket']\n",
    "        #host = ConfigSectionMap(\"Part_1\")['HOST']\n",
    "        # host='edgardatasets.s3-website-us-west-2.amazonaws.com'\n",
    "        # Creating a simple connection\n",
    "        conn = tinys3.Connection(S3_ACCESS_KEY,S3_SECRET_KEY)\n",
    "\n",
    "        # Uploading a single file\n",
    "        f = open(\"Part_2_log_datasets.zip\",'rb')\n",
    "        conn.upload(\"Part_2_log_datasets.zip\",f,BUCKET_NAME)  \n",
    "        \n",
    "get_data_obj=GetData()\n",
    "merged_dataframe=get_data_obj.fetch_year()\n",
    "#fetch the year for which the user wants logs\n",
    "#year = input('Enter the year for which you need to fetch the log files: ')\n",
    "#calling the function to generate dynamic URL\n",
    "\n",
    "#df=get_data_obj.generate_url(year)\n",
    "        \n",
    "#get_data_obj.create_zip_folder(\"Part_2_log_datasets\")\n",
    "#get_data_obj.upload_zip_to_s3(\"Part_2_log_datasets.zip\")\n",
    "\n",
    "class Process_and_analyse_data():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Retrieves and stores the urllib.urlopen object for a given url\n",
    "        \"\"\"\n",
    "    \n",
    "    def format_dataframe_columns(self):\n",
    "        #convert all the integer column in int format\n",
    "\n",
    "        df['zone'] = df['zone'].astype('int')\n",
    "        df['cik'] = df['cik'].astype('int')\n",
    "        df['code'] = df['code'].astype('int')\n",
    "        df['idx']=df['idx'].astype('int')\n",
    "        df['norefer']=df['norefer'].astype('int')\n",
    "        df['noagent']=df['noagent'].astype('int')\n",
    "        df['find']=df['find'].astype('int')\n",
    "        df['crawler']=df['crawler'].astype('int')\n",
    "        \n",
    "        #replacing empty strings with NaN \n",
    "        df.replace(r'\\s+', np.nan, regex=True)\n",
    "        self.handle_nan_values()\n",
    "        \n",
    "    def handle_nan_values(self):\n",
    "        \n",
    "        #replace all ip column NaN value by a default ip address \n",
    "        df[\"ip\"].fillna(\"255.255.255.255\", inplace=True)\n",
    "\n",
    "        #perform forward fill to replace NaN values by fetching the next valid value\n",
    "        df[\"date\"].fillna(method='ffill')\n",
    "\n",
    "        #perform backward fill to replace NaN values by backpropagating and fetching the previous valid value\n",
    "        df[\"time\"].fillna(method='bfill')\n",
    "\n",
    "        #replace all zone column NaN values by 'Not Available' extension\n",
    "        df[\"zone\"].fillna(\"Not Available\", inplace=True)\n",
    "\n",
    "        #replace all extension column NaN values by default extension\n",
    "        df[\"extention\"].fillna(\"-index.htm\", inplace=True)\n",
    "\n",
    "        #replace all size column NaN values by 0 and convert the column into integer \n",
    "        df[\"size\"].fillna(0, inplace=True)\n",
    "        df['size'] = df['size'].astype('int')\n",
    "\n",
    "        #replace all user agent column NaN values by the default value 1 (no user agent)\n",
    "        df[\"noagent\"].fillna(\"Not Applicable\", inplace=True)\n",
    "\n",
    "        #replace all find column NaN values by the default value 0 (no character strings found)\n",
    "        df[\"find\"].fillna(0, inplace=True)\n",
    "\n",
    "        #replace all broser column NaN values by a string\n",
    "        df[\"browser\"].fillna(\"Not Available\", inplace=True)\n",
    "        \n",
    "        # if the value in idx column is missing, check the value of the extension column, if its \"-index.html\" set the column's value 1 else 0\n",
    "        count=0\n",
    "        for i in df['idx']:\n",
    "            if(np.isnan(i)):\n",
    "                if(df['extension'][count]==\"-index.htm\"):\n",
    "                    i=1\n",
    "                else:\n",
    "                    i=0\n",
    "            count=count+1\n",
    "\n",
    "        # if the value of norefer column is missing, check the value of the find column, if it is 0, set the value 1, else it set it 0\n",
    "        counter=0\n",
    "        for i in df['norefer']:\n",
    "            if(np.isnan(i)):\n",
    "                if(df[\"find\"][counter]==0):\n",
    "                    i=1\n",
    "                else:\n",
    "                    i=0\n",
    "            counter=counter+1\n",
    "\n",
    "        # if the value of crawler is missing, check the value of the code, if it is 404 set it as 1 else 0\n",
    "        count_position=0\n",
    "        for i in df['crawler']:\n",
    "            if(np.isnan(i)):\n",
    "                if(df[\"code\"][count_position]==404):\n",
    "                    i=1\n",
    "                else:\n",
    "                    i=0\n",
    "            count_position=count_position+1\n",
    "        self.fetch_company_name_from_cik()\n",
    "    \n",
    "    def fetch_company_name_from_cik(self):\n",
    "        #we found a list of CIK their company names from a EDGAR's github repository. We are fetching this information to gain the information about company name\n",
    "        company_df = pd.read_csv('CIK-mapping.csv') \n",
    "        #renaming column, so that both the dataframes can be merged on the common column\n",
    "        company_df = company_df.rename(columns={'CIK': 'cik'})\n",
    "        company_df['cik'] = company_df['cik'].astype('int')\n",
    "        #merging both the dataframes\n",
    "        merged_df_cik_company_name= df.join(company_df, on='cik', how='left', rsuffix=\"_review\")\n",
    "        #merged_df_cik_company_name=pd.merge(df, company_df, on='cik', how='left')\n",
    "        #merged_df_cik_company_name=pd.merge(df,company_df, left_on='cik',right_on='CIK' )\n",
    "        print(merged_df_cik_company_name.head(10))\n",
    "        \n",
    "    def identify_cik_accession_number_anomaly(self):\n",
    "        #insert a column to check CIK, Accession number discripancy\n",
    "        df.insert(6, \"CIK_Accession_Anamoly_Flag\", \"N\")\n",
    "                \n",
    "        #check if CIK and Accession number match. The Accession number is divided into three parts, CIK-Year-Number_of_filings_listed.\n",
    "        #the first part i.e the CIK must match with the CIK column. If not, there exists an anomaly\n",
    "\n",
    "        count=0;\n",
    "        print(\"Creating CIK_Accession_Anomaly_Flag column\")\n",
    "        for i in df['accession']:\n",
    "            #fetch the CIK number from the accession number and convert it into integer\n",
    "            list_of_fetched_cik_from_accession=[(int(i.split(\"-\")[0]))]\n",
    "\n",
    "            #check if the CIK number from the column and CIK number fetched from the accession number are equal\n",
    "            if(df['cik'][count]!=list_of_fetched_cik_from_accession):\n",
    "                df['CIK_Accession_Anamoly_Flag'][count]=\"Y\"\n",
    "\n",
    "            count=count+1\n",
    "        print(\"Done\")\n",
    "        print(df.head(10))\n",
    "        \n",
    "    def get_file_name_from_extension():\n",
    "        df.insert(7, \"filename\", \"\")\n",
    "        #Extension rule: if the file name is missing and only the file extension is present, then the file name is document accession number\n",
    "        count=0\n",
    "        for i in df[\"extention\"]:\n",
    "            if(i==\".txt\"):\n",
    "                # if the value in extension is only .txt, fetch the accession number and append accession number to .txt\n",
    "                #list_of_fetched_cik_from_accession=int(((df2[\"accession\"].str.split(\"-\")[count])[0]))\n",
    "                #print((df[\"accession\"]).astype(str))\n",
    "                #list_of_fetched_cik_from_accession=int(df[\"accession\"])\n",
    "                df[\"filename\"][count]=(df[\"accession\"][count])+\".txt\" \n",
    "            else:\n",
    "                df[\"filename\"][count]=i\n",
    "            count=count+1\n",
    "        \n",
    "        \n",
    "get_data_obj=GetData()\n",
    "df=get_data_obj.getDataFrame()\n",
    "process_data_obj=Process_and_analyse_data()\n",
    "process_data_obj.format_dataframe_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#insert a column to check CIK, Accession number discripancy\n",
    "df.insert(6, \"CIK_Accession_Anamoly_Flag\", \"N\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#check if CIK and Accession number match. The Accession number is divided into three parts, CIK-Year-Number_of_filings_listed.\n",
    "#the first part i.e the CIK must match with the CIK column. If not, there exists an anomaly\n",
    "\n",
    "count=0;\n",
    "print(\"Creating CIK_Accession_Anomaly_Flag column\")\n",
    "for i in df['accession']:\n",
    "    #fetch the CIK number from the accession number and convert it into integer\n",
    "    list_of_fetched_cik_from_accession=[(int(i.split(\"-\")[0]))]\n",
    "    \n",
    "    #check if the CIK number from the column and CIK number fetched from the accession number are equal\n",
    "    if(df['cik'][count]!=list_of_fetched_cik_from_accession):\n",
    "        df['CIK_Accession_Anamoly_Flag'][count]=\"Y\"\n",
    "        \n",
    "    count=count+1\n",
    "print(\"Done\")\n",
    "#print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#merge both the dataframe using the CIK,cik as common column\n",
    "\n",
    "# read csv from source\n",
    "company_df = pd.read_csv('CIK-mapping.csv') \n",
    "\n",
    "merged_df=pd.merge(company_df, df, left_on='CIK',right_on='cik' )\n",
    "#merged_df.head()\n",
    "#merged_df.loc[merged_df['cik']==1438823]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.insert(7, \"filename\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Extension rule: if the file name is missing and only the file extension is present, then the file name is document accession number\n",
    "count=0\n",
    "for i in df[\"extention\"]:\n",
    "    if(i==\".txt\"):\n",
    "        # if the value in extension is only .txt, fetch the accession number and append accession number to .txt\n",
    "        #list_of_fetched_cik_from_accession=int(((df2[\"accession\"].str.split(\"-\")[count])[0]))\n",
    "        #print((df[\"accession\"]).astype(str))\n",
    "        #list_of_fetched_cik_from_accession=int(df[\"accession\"])\n",
    "        df[\"filename\"][count]=(df[\"accession\"][count])+\".txt\" \n",
    "    else:\n",
    "        df[\"filename\"][count]=i\n",
    "    count=count+1\n",
    "#print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ConfigSectionMap(\"Part_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
